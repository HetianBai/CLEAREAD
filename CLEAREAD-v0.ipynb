{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T03:52:26.171678Z",
     "start_time": "2020-07-30T03:52:20.825389Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from pattern.en import lemma\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import uuid\n",
    "import csv\n",
    "import phrasemachine\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request\n",
    "import difflib\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from wechatpy.client import WeChatClient\n",
    "import random\n",
    "import http.client\n",
    "import stardict\n",
    "from stardict import LemmaDB\n",
    "lemma = LemmaDB()\n",
    "lemma.load('lemma.en.txt')\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T03:52:26.183651Z",
     "start_time": "2020-07-30T03:52:26.174671Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"master_word_list.json\",'r') as load_f:\n",
    "    master_word_dict = json.load(load_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T03:52:26.271420Z",
     "start_time": "2020-07-30T03:52:26.186641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Easy word list\n",
    "CET4_list = []\n",
    "\n",
    "with open('CET-4_Vocab_v2.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        line=line.lower()\n",
    "        if line.startswith(('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z')):\n",
    "            word = re.findall('([a-z]+)',line)\n",
    "            CET4_list.append(word[0].lower().rstrip())\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "highschool_list = []\n",
    "\n",
    "with open('Highschool_Vocab.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        highschool_list.append(line.lower().rstrip())  \n",
    "\n",
    "middleschool_list = []\n",
    "\n",
    "with open('MiddleSchool_Vocab.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        middleschool_list.append(line.lower().rstrip())        \n",
    "        \n",
    "        \n",
    "elementary_list = []\n",
    "\n",
    "with open('Elementary_Vocab.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        elementary_list.append(line.lower().rstrip())\n",
    "\n",
    "easy_word_set = set(CET4_list) | set(highschool_list) | set(elementary_list) | set(middleschool_list)\n",
    "easy_word_list = list(easy_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T03:52:26.324271Z",
     "start_time": "2020-07-30T03:52:26.276400Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma_easy_word_list = []\n",
    "\n",
    "for i in range(0,len(easy_word_list)):\n",
    "    lemma_word = lemma.get(easy_word_list[i], reverse = False)\n",
    "    if lemma_word == None:\n",
    "        lemma_easy_word_list.append(easy_word_list[i])\n",
    "    else:\n",
    "        for j in range(0,len(lemma_word)):\n",
    "            lemma_easy_word_list.append(lemma_word[j])\n",
    "        lemma_easy_word_list.append(easy_word_list[i])\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T03:52:41.021101Z",
     "start_time": "2020-07-30T03:52:26.327264Z"
    }
   },
   "outputs": [],
   "source": [
    "# word category\n",
    "GMAT_list = []\n",
    "\n",
    "with open('GMAT_edited.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        GMAT_list.append(line.lower().rstrip())\n",
    "    \n",
    "stardict = pd.read_csv(\"stardict.csv\",encoding = 'utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T03:53:05.983777Z",
     "start_time": "2020-07-30T03:52:41.054016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the name of the bookSomething I've Been Meaning to - Alice Munro\n"
     ]
    }
   ],
   "source": [
    "bookname = input('enter the name of the book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T04:00:41.462005Z",
     "start_time": "2020-07-30T03:53:06.790892Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(bookname + \".txt\", 'r',encoding='unicode_escape') as file:\n",
    "    text = file.read().rstrip()\n",
    "\n",
    "# remove puntuation\n",
    "word_list_1 = re.findall(\"[a-zA-Z]+\\'?\\-?\\'?[a-zA-Z]+\", text)\n",
    "\n",
    "\n",
    "#remove names\n",
    "name_list = []\n",
    "for i in word_list_1:\n",
    "    if i.startswith(('A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z')) and i.lower() not in word_list_1:\n",
    "        name_list.append(i)\n",
    "\n",
    "\n",
    "word_list = []\n",
    "for i in word_list_1:\n",
    "    if i not in name_list:\n",
    "        word_list.append(i.lower())\n",
    "            \n",
    "#Lemmatization\n",
    "#LemmaDB can't lemma didn't or wouldn't, etc.\n",
    "lemma_word_list = []\n",
    "\n",
    "for i in range(0,len(word_list)):\n",
    "    lemma_word = lemma.get(word_list[i], reverse = True)\n",
    "    if lemma_word == None:\n",
    "        lemma_word_list.append(word_list[i])\n",
    "    else:\n",
    "        lemma_word_list.append(lemma_word[0])\n",
    "\n",
    "\n",
    "#select unfamiliar words\n",
    "raw_word_list = []\n",
    "for word in lemma_word_list:\n",
    "    if word not in lemma_easy_word_list:\n",
    "        raw_word_list.append(word)\n",
    "    else:\n",
    "        continue    \n",
    "\n",
    "\n",
    "#count the word\n",
    "word_count = pd.value_counts(raw_word_list)\n",
    "data = pd.DataFrame({'Vocabulary':word_count.index, 'counts':word_count.values})\n",
    "        \n",
    "# point out where the replicate comes from\n",
    "def get_key (dict, value):\n",
    "    return [k for k, v in dict.items() if v == value]\n",
    "\n",
    "\n",
    "for i in range(0,len(word_count.index)):\n",
    "    for j in list(master_word_dict.values()):\n",
    "        if data.loc[i,'Vocabulary'] in j:\n",
    "            data.loc[i,'Replicate'] = get_key(master_word_dict,j)\n",
    "        else:\n",
    "            pass\n",
    "for i in range(0,len(word_count.index)):\n",
    "    try:\n",
    "        category = re.findall(\"[a-z]+\\-?[a-z]+\", stardict.loc[stardict['word']== data.loc[i,'Vocabulary'] ,'tag'].to_string())\n",
    "        if data.loc[i,'Vocabulary'] in GMAT_list:\n",
    "            category.append('gmat')\n",
    "        cate = \",\".join(category)\n",
    "        data.loc[i,'Cate'] = cate\n",
    "    except IndexError:\n",
    "        pass     \n",
    "data.to_csv( bookname + \"vocabularycounts_v0.csv\",index=False,header=True)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "603px",
    "left": "1576px",
    "top": "215px",
    "width": "235.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
