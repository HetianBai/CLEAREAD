{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T15:13:35.325528Z",
     "start_time": "2020-06-28T15:13:30.627152Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from pattern.en import lemma\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import uuid\n",
    "import csv\n",
    "import phrasemachine\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request\n",
    "import difflib\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from wechatpy.client import WeChatClient\n",
    "import random\n",
    "import http.client\n",
    "import stardict\n",
    "from stardict import LemmaDB\n",
    "lemma = LemmaDB()\n",
    "lemma.load('lemma.en.txt')\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T15:17:29.088972Z",
     "start_time": "2020-06-28T15:17:29.073989Z"
    }
   },
   "outputs": [],
   "source": [
    "master_word_dict = {}\n",
    "\n",
    "HP1_word_final = []\n",
    "csvFile = open(\"HP1vocabularycounts_v2.5.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP1_word_final.append(i[0])\n",
    "master_word_dict['HP1'] = HP1_word_final\n",
    "\n",
    "HP2_word_final = []\n",
    "csvFile = open(\"HP2vocabularycounts_v2.5.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP2_word_final.append(i[0])\n",
    "master_word_dict['HP2'] = HP2_word_final\n",
    "\n",
    "HP3_word_final = []\n",
    "csvFile = open(\"HP3vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP3_word_final.append(i[0])\n",
    "master_word_dict['HP3'] = HP3_word_final\n",
    "\n",
    "HP4_word_final = []\n",
    "csvFile = open(\"HP4vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP4_word_final.append(i[0])\n",
    "master_word_dict['HP4'] = HP4_word_final\n",
    "\n",
    "HP5_word_final = []\n",
    "csvFile = open(\"HP5vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP5_word_final.append(i[0])\n",
    "master_word_dict['HP5'] = HP5_word_final\n",
    "\n",
    "HP6_word_final = []\n",
    "csvFile = open(\"HP6vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP6_word_final.append(i[0])\n",
    "master_word_dict['HP6'] = HP6_word_final\n",
    "\n",
    "HP7_word_final = []\n",
    "csvFile = open(\"HP7vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP7_word_final.append(i[0])\n",
    "master_word_dict['HP7'] = HP7_word_final\n",
    "\n",
    "Heartisthelonelyhunter_word_final = []\n",
    "csvFile = open(\"TheHeartisthelonelyhuntervocabularycounts_v2.5.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    Heartisthelonelyhunter_word_final.append(i[0])\n",
    "master_word_dict['Heart is the Lonely Hunter'] = Heartisthelonelyhunter_word_final\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy word list\n",
    "CET4_list = []\n",
    "\n",
    "with open('CET-4_Vocab.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        if line.startswith(('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z')):\n",
    "            word = re.findall('([a-z]+)',line)\n",
    "            CET4_list.append(word[0])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "highschool_list = []\n",
    "\n",
    "with open('Highschool_Vocab.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        highschool_list.append(line.lower().rstrip())  \n",
    "\n",
    "elementary_list = []\n",
    "\n",
    "with open('Elementary_Vocab.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        elementary_list.append(line.lower().rstrip())\n",
    "\n",
    "easy_word_set = set(CET4_list) | set(highschool_list) | set(elementary_list)\n",
    "easy_word_list = list(easy_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T15:09:41.485745Z",
     "start_time": "2020-06-28T15:09:37.776192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the name of the bookTheHeartisthelonelyhuntervocabularycounts_v2.5\n"
     ]
    }
   ],
   "source": [
    "bookname = input('enter the name of the book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T15:09:43.480395Z",
     "start_time": "2020-06-28T15:09:43.476383Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(bookname + \".txt\", 'r',encoding='utf-8') as file:\n",
    "    text = file.read().rstrip().lower()\n",
    "\n",
    "# remove puntuation\n",
    "word_list = re.findall(\"[a-z]+\\-?[a-z]+\", text)\n",
    "\n",
    "#Lemmatization\n",
    "#LemmaDB can't lemma didn't or wouldn't, etc.\n",
    "lemma_word_list = []\n",
    "\n",
    "for i in range(0,len(word_list)):\n",
    "    lemma_word = lemma.get(word_list[i], reverse = True)\n",
    "    if lemma_word == None:\n",
    "        lemma_word_list.append(word_list[i])\n",
    "    else:\n",
    "        lemma_word_list.append(lemma_word[0])\n",
    "\n",
    "\n",
    "#select unfamiliar words\n",
    "raw_word_list = []\n",
    "for word in lemma_word_list:\n",
    "    if word not in lemma_easy_word_list:\n",
    "        raw_word_list.append(word)\n",
    "    else:\n",
    "        continue    \n",
    "\n",
    "\n",
    "#count the word\n",
    "word_count = pd.value_counts(raw_word_clean_list)\n",
    "data = pd.DataFrame({'Vocabulary':word_count.index, 'counts':word_count.values})\n",
    "        \n",
    "# point out where the replicate comes from\n",
    "def get_key (dict, value):\n",
    "    return [k for k, v in dict.items() if v == value]\n",
    "\n",
    "\n",
    "for i in range(0,len(word_count.index)):\n",
    "    for j in list(master_word_dict.values()):\n",
    "        if data.loc[i,'Vocabulary'] in j:\n",
    "            data.loc[i,'Replicate'] = get_key(master_word_dict,j)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "data.to_csv( bookname + \"vocabularycounts_v0.csv\",index=False,header=True)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "603px",
    "left": "1576px",
    "top": "215px",
    "width": "235.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
