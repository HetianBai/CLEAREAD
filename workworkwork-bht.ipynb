{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from pattern.en import lemma\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import uuid\n",
    "import csv\n",
    "import phrasemachine\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request\n",
    "import difflib\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from wechatpy.client import WeChatClient\n",
    "import random\n",
    "import http.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What I should do next? \n",
    "#### Consider what is a better way of lemma (Now I have to lemma the replicate list to get a better result and it should be fixed)***\n",
    "#### Consider how to build a database or container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_word_dict = {}\n",
    "\n",
    "HP1_word_final = []\n",
    "csvFile = open(\"HP1vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP1_word_final.append(lemma(i[0]))\n",
    "master_word_dict['HP1'] = HP1_word_final\n",
    "\n",
    "HP2_word_final = []\n",
    "csvFile = open(\"HP2vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP2_word_final.append(lemma(i[0]))\n",
    "master_word_dict['HP2'] = HP2_word_final\n",
    "\n",
    "HP3_word_final = []\n",
    "csvFile = open(\"HP3vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP3_word_final.append(lemma(i[0]))\n",
    "master_word_dict['HP3'] = HP3_word_final\n",
    "\n",
    "HP4_word_final = []\n",
    "csvFile = open(\"HP4vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP4_word_final.append(lemma(i[0]))\n",
    "master_word_dict['HP4'] = HP4_word_final\n",
    "\n",
    "HP5_word_final = []\n",
    "csvFile = open(\"HP5vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP5_word_final.append(lemma(i[0]))\n",
    "master_word_dict['HP5'] = HP5_word_final\n",
    "\n",
    "HP6_word_final = []\n",
    "csvFile = open(\"HP6vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP6_word_final.append(lemma(i[0]))\n",
    "master_word_dict['HP6'] = HP6_word_final\n",
    "\n",
    "HP7_word_final = []\n",
    "csvFile = open(\"HP7vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    HP7_word_final.append(lemma(i[0]))\n",
    "master_word_dict['HP7'] = HP7_word_final\n",
    "\n",
    "Heartisthelonelyhunter_word_final = []\n",
    "csvFile = open(\"The Heart is a Lonely Hunter_VocabCount_v4.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    Heartisthelonelyhunter_word_final.append(lemma(i[0]))\n",
    "master_word_dict['Heart is the Lonely Hunter'] = Heartisthelonelyhunter_word_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Clean the text, lemma words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7.Harry Potter and the Deathly Hallows.txt\", 'r',encoding='unicode_escape') as file:\n",
    "    text = file.read().rstrip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove puntuation\n",
    "word_list = re.findall(\"[a-z]+\\-?\\'?[a-z]+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "lemma_word_list = []\n",
    "for i in range(0,len(word_list)):\n",
    "    word = lemma(word_list[i])\n",
    "    lemma_word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CET4_list = []\n",
    "\n",
    "with open('CET-4_Vocab.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        if line.startswith(('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z')):\n",
    "            word = re.findall('([a-z]+)',line)\n",
    "            CET4_list.append(word[0])\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "highschool_list = []\n",
    "\n",
    "with open('Highschool_Vocab.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        highschool_list.append(line.lower().rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementary_list = []\n",
    "\n",
    "with open('Elementary_Vocab.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        elementary_list.append(line.lower().rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = []\n",
    "\n",
    "with open('name.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        name_list.append(line.lower().rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_word_list = set(CET4_list) | set(highschool_list) | set(elementary_list)\n",
    "easy_word_list = list(easy_word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize easy word list\n",
    "lemma_easy_word_list = []\n",
    "for i in easy_word_list:\n",
    "    word = lemma(i)\n",
    "    lemma_easy_word_list.append(word)\n",
    "    \n",
    "for i in name_list:\n",
    "    lemma_easy_word_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select unfamiliar words\n",
    "raw_word_list = []\n",
    "for word in lemma_word_list:\n",
    "    if word not in lemma_easy_word_list:\n",
    "        raw_word_list.append(word)\n",
    "    else:\n",
    "        continue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean '\n",
    "raw_word_clean_list = []\n",
    "for word in raw_word_list:\n",
    "    try:\n",
    "        word_clean = re.findall(\"[a-z]+\\-?[a-z]+\", word)[0]\n",
    "        raw_word_clean_list.append(word_clean)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    continue\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the word\n",
    "word_count = pd.value_counts(raw_word_clean_list)\n",
    "data = pd.DataFrame({'Vocabulary':word_count.index, 'counts':word_count.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point out where the replicate comes from\n",
    "def get_key (dict, value):\n",
    "    return [k for k, v in dict.items() if v == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(word_count.index)):\n",
    "    for j in list(master_word_dict.values()):\n",
    "        if data.loc[i,'Vocabulary'] in j:\n",
    "            data.loc[i,'Replicate'] = get_key(master_word_dict,j)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "data.to_csv(\"HP7vocabularycounts_v0.csv\",index=False,header=True)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Translation & Phonetics & Characteristic & Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app_id = '761150e6'\n",
    "app_key = '9c547ac3ff97fad8a7e366f8129d6a2c'\n",
    "\n",
    "word_id = 'better'\n",
    "strictMatch = 'false'\n",
    "\n",
    "url = 'https://od-api.oxforddictionaries.com:443/api/v2/entries/en-gb/' + word_id.lower() + '?strictMatch=' + strictMatch;\n",
    "\n",
    "r = requests.get(url, headers = {'app_id': app_id, 'app_key': app_key})\n",
    "pron = r.json()[\"results\"][0]['lexicalEntries'][0]['entries'][0]['pronunciations'][0]['phoneticSpelling']\n",
    "cate = r.json()[\"results\"][0]['lexicalEntries'][0]['lexicalCategory']['text']\n",
    "defin = r.json()[\"results\"][0]['lexicalEntries'][0]['entries'][0]['senses'][0]['definitions'][0]\n",
    "print(pron,cate,defin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = r.json()\n",
    "b = a[\"results\"][0]['lexicalEntries'][0]['entries'][0]['pronunciations'][0]['phoneticSpelling']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = '5c864748'\n",
    "app_key = '11013df45a50b3d7960bdcb4fe77af01'\n",
    "\n",
    "language = 'en'\n",
    "word_id = 'better'\n",
    "strictMatch = 'false'\n",
    "\n",
    "url = 'https://od-api.oxforddictionaries.com:443/api/v2/entries/' + language + '/' + word_id.lower() + '?fields=' + fields + '&strictMatch=' + strictMatch;\n",
    "\n",
    "r = requests.get(url, headers = {'app_id': app_id, 'app_key': app_key})\n",
    "pron = r.json()\n",
    "pron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = r.json()\n",
    "d = c[\"results\"][0]['lexicalEntries'][0]['lexicalCategory']['text']\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youdao translation for plan B\n",
    "youdao_url = 'https://openapi.youdao.com/api'  \n",
    "\n",
    "input_text = \"better\"\n",
    "translate_text = \"dream\"\n",
    "time_curtime = int(time.time())   # 秒级时间戳获取\n",
    "app_id = \"15a1da979e8d68c0\"   # 应用id\n",
    "uu_id = uuid.uuid4()   # 随机生成的uuid数，为了每次都生成一个不重复的数。\n",
    "app_key = \"SSfsA7MhSvqvG79kKVzGx875O2ukapUW\"   # 应用密钥\n",
    "\n",
    "sign = hashlib.sha256((app_id + input_text + str(uu_id) + str(time_curtime) + app_key).encode('utf-8')).hexdigest()   # sign生成\n",
    "\n",
    "\n",
    "data = {\n",
    "    'q':translate_text,   # 翻译文本\n",
    "    'from':\"en\",   # 源语言\n",
    "    'to':\"zh-CHS\",   # 翻译语言\n",
    "    'appKey':app_id,   # 应用id\n",
    "    'salt':uu_id,   # 随机生产的uuid码\n",
    "    'sign':sign,   # 签名\n",
    "    'signType':\"v3\",   # 签名类型，固定值\n",
    "    'curtime':time_curtime,   # 秒级时间戳\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "r = requests.get(youdao_url, params = data).json()\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 the full version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = open(\"HP7vocabularycounts_v2.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API id and key\n",
    "app_id = '491e882b'\n",
    "app_key = 'a0b004ca6b5f7f4e820832f3dc934355'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMAT_list = []\n",
    "\n",
    "with open('GMAT_edited.txt','r',encoding='utf-8') as fhandle:\n",
    "    for line in fhandle:\n",
    "        GMAT_list.append(line.lower().rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\apps\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "word_pron_char_defin_cate = []   \n",
    "df = pd.read_csv(\"stardict.csv\",encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reader:\n",
    "    category = re.findall(\"[a-z]+\\-?[a-z]+\", df.loc[df['word']== i[0] ,'tag'].to_string())\n",
    "    if i[0] in GMAT_list:\n",
    "        category.append('gmat')\n",
    "    cate = \",\".join(category)\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        word_id = i[0]\n",
    "        strictMatch = 'false'\n",
    "\n",
    "        url = 'https://od-api.oxforddictionaries.com:443/api/v2/entries/en-gb/' + word_id.lower() + '?strictMatch=' + strictMatch;\n",
    "\n",
    "        r = requests.get(url, headers = {'app_id': app_id, 'app_key': app_key})\n",
    "        pron = r.json()[\"results\"][0]['lexicalEntries'][0]['entries'][0]['pronunciations'][0]['phoneticSpelling']\n",
    "        char = r.json()[\"results\"][0]['lexicalEntries'][0]['lexicalCategory']['text']\n",
    "        defin = r.json()[\"results\"][0]['lexicalEntries'][0]['entries'][0]['senses'][0]['definitions'][0]\n",
    "        word_pron_char_defin_cate.append([i[0],'/'+pron+'/',char,defin,cate])\n",
    "        time.sleep(10)\n",
    "    except KeyError :\n",
    "        pass\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in word_pron_char_defin_cate:\n",
    "    if i[2] == 'Noun':\n",
    "        i[2] = 'n.'\n",
    "    if i[2] == 'Adjective':\n",
    "        i[2] = 'adj.'\n",
    "    if i[2] == 'Verb':\n",
    "        i[2] = 'v.'\n",
    "    if i[2] == 'Adverb':\n",
    "        i[2] = 'adv.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baidu API for EN-CN Translate\n",
    "appid = '20200627000507736'  # 填写你的appid\n",
    "secretKey = 'G3ghonyD9dfI39NnFtGN'  # 填写你的密钥\n",
    "\n",
    "httpClient = None\n",
    "myurl = '/api/trans/vip/translate'\n",
    "\n",
    "fromLang = 'en'   #原文语种\n",
    "toLang = 'zh'   #译文语种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_pron_char_defin_cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_pron_char_defin_cate_cn = []\n",
    "for i in word_pron_char_defin_cate:\n",
    "    salt = random.randint(32768, 65536)\n",
    "    sign = appid + i[0] + str(salt) + secretKey\n",
    "    sign = hashlib.md5(sign.encode()).hexdigest()\n",
    "    myurl = myurl + '?appid=' + appid + '&q=' + urllib.parse.quote(i[0]) + '&from=' + fromLang + '&to=' + toLang + '&salt=' + str(salt) + '&sign=' + sign\n",
    "\n",
    "    try:\n",
    "        httpClient = http.client.HTTPConnection('api.fanyi.baidu.com')\n",
    "        httpClient.request('GET', myurl)\n",
    "\n",
    "        # response是HTTPResponse对象\n",
    "        response = httpClient.getresponse()\n",
    "        result_all = response.read().decode(\"utf-8\")\n",
    "        result = json.loads(result_all)['trans_result'][0]['dst']\n",
    "        i.append(result)\n",
    "        word_pron_char_defin_cate_cn.append(i)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open('HP7vocabularycounts_v3.txt', 'w',encoding='utf-8')\n",
    "for i in word_pron_char_defin_cate_cn:\n",
    "    for j in i:\n",
    "        fileObject.write(j)\n",
    "        fileObject.write('|')\n",
    "    fileObject.write('\\n')\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Find phrases in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(word_list)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "sorted(bigram for bigram, score in scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Omnibus Crime Control and Safe Streets Act of 1968 was signed into law by President Lyndon B. Johnson.so as James\"\n",
    "phrasemachine.get_phrases(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list=[['bring','out'],['look','in','the','eye'],['dream','of'],['want','to'],['had','it','in']]\n",
    "raw_phrase_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 full version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pharase with space\n",
    "csvFile = open(\"Phrase Book With Space.csv\", \"r\",encoding = 'utf-8-sig')\n",
    "reader = csv.reader(csvFile)\n",
    "phrase = []\n",
    "phrase_with_space_list = []\n",
    "for i in reader:\n",
    "    phrase.append(i)\n",
    "for i in phrase:\n",
    "    i = [j for j in i if j !=  '']\n",
    "    phrase_with_space_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pharase without space\n",
    "csvfile = open(\"Phrase Book No Space_v2.csv\", \"r\",encoding = 'utf-8-sig')\n",
    "reader = csv.reader(csvfile)\n",
    "phrase = []\n",
    "phrase_no_space_list = []\n",
    "for i in reader:\n",
    "    phrase.append(i)\n",
    "for i in phrase:\n",
    "    i = [j for j in i if j !=  '']\n",
    "    phrase_no_space_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select pharses appeared with spaces\n",
    "raw_phrase_with_space_list = []\n",
    "\n",
    "for i in range(0,len(word_list)):\n",
    "    for j in phrase_with_space_list:\n",
    "        try:\n",
    "            if word_list[i] == j[0] and j[-1] in word_list[i+1:i+6]:\n",
    "                if j[-2] in word_list[i+1:i+5]:\n",
    "                    raw_phrase_with_space_list.append(j)\n",
    "                else:\n",
    "                    continue                \n",
    "            else:\n",
    "                continue\n",
    "        except IndexError:\n",
    "            pass\n",
    "        continue\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select pharses appeared without spaces\n",
    "raw_phrase_no_space_list = []\n",
    "correct = 0\n",
    "for i in range(0,len(word_list)):\n",
    "    for j in phrase_no_space_list:\n",
    "        try:\n",
    "            length = len(j)\n",
    "            for h in range(0,length):\n",
    "                if word_list[i+h] == j[h]:\n",
    "                    correct += 1\n",
    "            if correct == length:\n",
    "                raw_phrase_no_space_list.append(j)\n",
    "                correct = 0\n",
    "            else:\n",
    "                correct = 0\n",
    "        except IndexError:\n",
    "            pass\n",
    "        continue\n",
    "            \n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in raw_phrase_no_space_list:\n",
    "    raw_phrase_with_space_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_count = pd.value_counts(raw_phrase_with_space_list)\n",
    "data = pd.DataFrame({'Phrase':phrase_count.index, 'counts':phrase_count.values})\n",
    "data.to_csv(\"HP2phrasecounts.csv\",index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scrape Booklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bookerprize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = []\n",
    "for i in range(2010,2020):\n",
    "    url.append('https://thebookerprizes.com/fiction/backlist/'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_author_list = []\n",
    "for i in url:\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    req = Request(i,headers=hdr)\n",
    "    html= urlopen(req)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    shortlist = soup.find('div',class_=\"flexslider bookCarousel\")\n",
    "    name_author=shortlist.find_all('h4',class_=\"title\")\n",
    "    for k in name_author:\n",
    "        try:\n",
    "            name = k.get_text()\n",
    "            author = re.findall('\\-by\\-(\\S+)',k.a['href'])[0].replace('-',' ')\n",
    "            name_author_list.append([name,author])\n",
    "        except IndexError:\n",
    "            pass\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_author_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word = 'https://mmbiz.qpic.cn/mmbiz_jpg/B0OT7bl3qeH6X79DA4grDtmR9MYSCWzcaJnXUm70lia7Ot4h9L21OZFUzeNic40wicevPiajFuXlf3K65PNgiaPbtzg/0?wx_fmt=jpeg'\n",
    "defination = 'https://mmbiz.qpic.cn/mmbiz_jpg/B0OT7bl3qeH6X79DA4grDtmR9MYSCWzcxUoRJ1YErqXYmesqTaxPnpJ7icDbyLat6tDUlQIeHFpo0fEdOcXJtFw/0?wx_fmt=jpeg'\n",
    "css = \"\"\"<section data-width=\"100%\" style=\"display: inline-block; width: 100%; font-size: -3px; background-image: url(&quot;\"\"\" + defination + \"\"\"&quot;); background-size: cover; vertical-align: top; background-attachment: scroll; box-sizing: border-box; text-align: center; background-position: 0% 0%; background-repeat: no-repeat; max-width: 100% !important;\">\n",
    "<svg data-width=\"100%\" enable-background=\"new 0 0 1067 212\" space=\"preserve\" style=\"display: inline-block;outline: none;width: 100%;margin-top: -1px;border-width: initial;border-style: none;border-color: initial;vertical-align: top;background-position: 0% 0%;background-repeat: no-repeat;background-size: 100%;background-attachment: scroll;background-image: url(&quot;\"\"\" + word + \"\"\"&quot;);-webkit-tap-highlight-color: transparent;\" viewBox=\"0 0 1067 212\" x=\"0px\" xml=\"\" y=\"0px\">\n",
    "<set attributeName=\"visibility\" from=\"visible\" to=\"hidden\" begin=\"click+0.5\"></set>\n",
    "<animate attributeName=\"opacity\" begin=\"click\" dur=\"0.5s\" values=\"1;0\" restart=\"never\" fill=\"freeze\">\n",
    "</animate>\n",
    "<set attributeName=\"visibility\" from=\"hidden\" to=\"visible\" begin=\"click+3\"></set>\n",
    "<animate attributeName=\"opacity\" begin=\"click+3\" dur=\"0.3s\" values=\"0;1\" restart=\"never\" fill=\"freeze\">\n",
    "</animate>\n",
    "</svg></section>\"\"\"\n",
    "print(css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CSS editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = {\n",
    "    \"appid\": \"wx72b0cc170a66fe7b\",\n",
    "    \"appsecret\": \"9e9ef69b54fd1688376dc82b130f4bf9\"\n",
    "}\n",
    "wx = WeChatClient(app.get(\"appid\"), app.get(\"appsecret\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_word_link = []\n",
    "pic_defin_link = []\n",
    "\n",
    "for i in range(1,26):\n",
    "    pic = \"D://\" + str(i) + \".JPG\"\n",
    "    pic_word_link.append(pic)\n",
    "\n",
    "\n",
    "for i in range(26,51):\n",
    "    pic = \"D://\" + str(i) + \".JPG\"\n",
    "    pic_defin_link.append(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D://26.JPG',\n",
       " 'D://27.JPG',\n",
       " 'D://28.JPG',\n",
       " 'D://29.JPG',\n",
       " 'D://30.JPG',\n",
       " 'D://31.JPG',\n",
       " 'D://32.JPG',\n",
       " 'D://33.JPG',\n",
       " 'D://34.JPG',\n",
       " 'D://35.JPG',\n",
       " 'D://36.JPG',\n",
       " 'D://37.JPG',\n",
       " 'D://38.JPG',\n",
       " 'D://39.JPG',\n",
       " 'D://40.JPG',\n",
       " 'D://41.JPG',\n",
       " 'D://42.JPG',\n",
       " 'D://43.JPG',\n",
       " 'D://44.JPG',\n",
       " 'D://45.JPG',\n",
       " 'D://46.JPG',\n",
       " 'D://47.JPG',\n",
       " 'D://48.JPG',\n",
       " 'D://49.JPG',\n",
       " 'D://50.JPG']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_word_url = []\n",
    "pic_defin_url = []\n",
    "\n",
    "for i in pic_word_link:\n",
    "    resp = wx.material.add(\"image\",open(i,\"rb\"))\n",
    "    pic_word_url.append(resp['url'])\n",
    "\n",
    "\n",
    "for i in pic_defin_link:\n",
    "    resp = wx.material.add(\"image\",open(i,\"rb\"))\n",
    "    pic_defin_url.append(resp['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pic_html = []\n",
    "\n",
    "for i in range(0,len(pic_word_link)):\n",
    "    css = \"\"\"<section data-width=\"100%\" style=\"display: inline-block; width: 100%; font-size: 0px; background-image: url(&quot;\"\"\" + pic_defin_link[i] + \"\"\"&quot;); background-size: cover; vertical-align: top; background-attachment: scroll; box-sizing: border-box; text-align: center; background-position: 0% 0%; background-repeat: no-repeat; max-width: 100% !important;\">\n",
    "<svg data-width=\"100%\" enable-background=\"new 0 0 1080 1080\" space=\"preserve\" style=\"display: inline-block;outline: none;width: 100%;margin-top: -1px;border-width: initial;border-style: none;border-color: initial;vertical-align: top;background-position: 0% 0%;background-repeat: no-repeat;background-size: 100%;background-attachment: scroll;background-image: url(&quot;\"\"\" + pic_word_link[i] + \"\"\"&quot;);-webkit-tap-highlight-color: transparent;\" viewBox=\"0 0 1080 1080\" x=\"0px\" xml=\"\" y=\"0px\">\n",
    "<set attributeName=\"visibility\" from=\"visible\" to=\"hidden\" begin=\"click+0.3\"></set>\n",
    "<animate attributeName=\"opacity\" begin=\"click\" dur=\"0.3s\" values=\"1;0\" restart=\"never\" fill=\"freeze\">\n",
    "</animate>\n",
    "<set attributeName=\"visibility\" from=\"hidden\" to=\"visible\" begin=\"click+3\"></set>\n",
    "<animate attributeName=\"opacity\" begin=\"click+3\" dur=\"0.3s\" values=\"0;1\" restart=\"never\" fill=\"freeze\">\n",
    "</animate>\n",
    "</svg>\n",
    "</section>\"\"\" \n",
    "    pic_html.append(css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fileObject = open('Wechat HTML.txt', 'w',encoding='utf-8')\n",
    "for i in pic_html:\n",
    "    fileObject.write(i)\n",
    "    fileObject.write('\\n')\n",
    "fileObject.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = open(\"The Heart is a Lonely Hunter_VocabCount_v4.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "match_word_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reader:\n",
    "    match_word = difflib.get_close_matches(i[0], set(word_list),10,0.2)\n",
    "    match_word_list.append(match_word)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"The Heart is a Lonely Hunter_VocabCount_v7.csv\",\"w\",newline = '') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(match_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prop', 'plop', 'pp', 'pulp', 'plup', 'happy']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difflib.get_close_matches('prop', ['plup','prop','happy','pp','plop','pulp'],10,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. StarDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stardict\n",
    "from stardict import LemmaDB\n",
    "lemma = LemmaDB()\n",
    "lemma.load('lemma.en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = lemma.word_stem(\"hasn't\")\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. wordlist 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"stardict.csv\",encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['word']=='abandon','tag'].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = open(\"The Heart is a Lonely Hunter_VocabCount_v4.csv\", \"r\")\n",
    "reader = csv.reader(csvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_category = []\n",
    "for i in reader:\n",
    "    category = re.findall(\"[a-z]+\\-?[a-z]+\", df.loc[df['word']== i[0] ,'tag'].to_string())\n",
    "    if i[0] in GMAT_list:\n",
    "        category.append('gmat')\n",
    "    word_category.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "json_handle = open('PEPChuZhong7_1.json', 'rb')\n",
    "json_string = json_handle.read()\n",
    "json_dict = json.loads(json_string)\n",
    "type(json_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Remove Replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_word_list = []\n",
    "with open(\"HPmaster word list.txt\", 'r',encoding='utf-8') as file:\n",
    "    for i in file:\n",
    "        master_word_list.append(i.rstrip())\n",
    "master_word_set = set(master_word_list)\n",
    "master_word_list = list(master_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP7_word_clean = []\n",
    "\n",
    "csvFile = open(\"HP7vocabularycounts_v4.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "for i in reader:\n",
    "    if i not in master_word_list:\n",
    "        HP7_word_clean.append(i)\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"HPVocabularycounts_v5.csv\",\"w\",newline = '') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(HP7_word_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
